APPROACH FOR IMPROVING PANORAMIC IMAGE SEGMENTATION QUALITY ON CONSTRUCTION SITES USING SAM 
Juho Han1, Sanghyeon Na1, Mingyun Kang1, Sebeen Yoon1, Taehoon Kim1
1 Seoul National University of Science and Technology, Seoul, Republic of Korea

Abstract 
Research on image analysis utilizing deep learning technology in construction sites is actively progressing. Building large datasets required to train deep learning models is a challenging task in construction sites. Therefore, this study proposes the utilization of the Segment Anything Model (SAM) as a method to enhance segmentation performance of the existing deep learning models without training datasets. We integrated SAM with Panoplane360 as the target model. The experimental results showed an improvement in MIoU from 0.696 when using only Panoplane360 to 0.714 with SAM integration. This method holds the potential to deliver more accurate results and analysis, particularly in construction tasks such as site monitoring, quantity estimation, progress management, quality control, and maintenance, where deep learning models are applied.
© 2024 The Authors. Published by Diamond Congress Ltd.
Peer-review under responsibility of the scientific committee of the Creative Construction Conference 2024.
Keywords: deep learning, image segmentation, panoramic image

1. Introduction
In the construction industry, images are utilized for various purposes, including site monitoring, quantity estimation, progress management, quality control, and maintenance. [1]. As the field of deep learning has evolved, there has been a growing body of research in the construction sector that utilizes images to train deep learning models [2-6]. To effectively use deep learning models, many images must be collected and labeled to build a large dataset.  However, construction sites are not freely accessible due to safety concerns, and image acquisition is challenging due to occlusions encountered during photography [7].  
To address these issues, a study was conducted to build a large-scale datasets of construction site, known as the Construction Instance Segmentation (CIS) Datasets [8].  50,000 images including ten object categories belonging to workers, machines, and materials were collected and labeled. However, the object categories are specific to prefabricated building construction sites and cannot be generalized to other construction sites. 
Furthermore, building large image datasets for construction sites is time-consuming and labor-intensive, as even if the required image data is collected, it still needs to be labeled. To address these issues, attempts have been made to shorten the labeling process for construction site image data. A study was conducted using CycleGAN to replace BIM images with real-world images and automatically acquire labeled synthetic images using spatial information from the BIM [9]. However, there is a limitation that the method can only be applied if the BIM model of the construction site is available.
Therefore, it is still difficult to train deep learning models by building large datasets from construction sites. In this situation, the Segment Anything Model (SAM) has been actively researched because it can show remarkable segmentation results without the need to construct large datasets. Chen et al. [10] proposed a novel network that effectively applies SAM for accurate automated hydraulic sampling. The proposed network outperforms SAM-Adapter and achieves performance comparable to mainstream fully supervised segmentation techniques in industrial applications. Giannakis et al. [11] proposed a flexible crater detection scheme (CDA) based on SAM. The proposed CDA performed equally well on different types of datasets and objects, and did not require additional labeled data for fine-tuning the SAM. 
This study proposes a method to improve the quality of segmentation results without building a large image dataset for panoramic images taken at construction sites. The proposed method consists of inputting the panoramic image to SAM and Panoplane360 to obtain the segmentation mask, and then assigning the label of Panoplane360 to the mask of SAM. 
This paper is organized as follows: Section 2 provides the reasons for selecting Panoramic Image as the target image and describes SAM and Panoplane360. Then, Section 3 describes the research method. Section 4 describes the evaluation image data and presents the evaluation results. Finally, Section 5 presents the conclusions, limitations, and further studies based on the findings in this paper.

2.  Literature Review
2.1. Panoramic Image
Panoramic images have the great advantage of being able to compress omnidirectional information into a single image. However, panoramic images are characterized by a high FOV, distorted phase, lack of relevant datasets, and the distribution of objects is not biased toward the center of the image [12]. So, it is quite challenging for panoramic images to perform deep learning-based tasks that are specific to general images. Therefore, this study aims to improve the quality of segmentation by targeting panoramic images.
2.2. Segment Anything Model
Segment Anything Model (SAM) is a foundation model for image segmentation developed by Meta AI in 2023 [13]. The main feature of SAM is that it enables zero-shot and few-shot learning by performing promptable segmentation tasks. A promptable segmentation task is a task that outputs a valid segmentation mask at any prompt, which means that even if the mask is ambiguous or points to multiple objects, it outputs a valid segmentation mask that points to only one object. SAM also built a large dataset, SA-1B, with a 1.1B mask. Compared to another large dataset (COCO, Open Images V5), SA-1B is characterized by a wider coverage of image corners. This means that SAM can easily segment not only objects in the center of the image, but also objects at the edges of the image when predicting the mask. Thanks to SAM's promptable segmentation task and SA-1B's spatial distribution of object centers, SAM can be used to segment object effectively without additional training.
2.3. Panoplane360 
Panoplane360 is a CNN-based deep learning model that segments panoramic images into vertical and horizontal planes as instance, then performs indoor 3D reconstruction based on this information [14]. Panoplane360 uses a divide-and-conquer strategy for effective plane instance segmentation. It rapidly generates masks by dividing pixels into surface orientation groupings and applying pixel embedding clustering to identify unique plane instances. In addition, yaw-invariant V-planar reparameterization enables the deep learning model to infer the orientation of the vertical plane without knowledge of the 360° camera yaw rotation.
Panoplane360 can easily and quickly obtain vertical and horizontal plane information of a construction site without any additional steps. However, the quality of 3D reconstruction is determined by the results of instance segmentation for vertical and horizontal planes, making it a model that requires improvement in segmentation quality.

3. Methodology
The method proposed in this study is shown in Fig. 1. First, a 360-degree panoramic image is passed through the Panoplane360 model and SAM model to extract segmentation masks. Next, overlap the two extracted masks and iteratively assign the most dominant label among the labels of the Panoplane360 mask part that overlaps with the object of the SAM mask. After that, only objects with more than 50% labels are kept improving the segmentation result.

3.1. Extract Masks
To automatically segment objects in the panoramic image, we performed automatic mask generation using the SAM (vit- h) model (Fig. 2).  Automatic mask generation generates points at a user-specified number per side of the image. In other words, it generates points inside the image at the square of the specified number by the user. Then, SAM inferred the mask based on the generated points and output the most valid segmentation mask based on the inferred mask according to the threshold. This makes it possible to input a panoramic image of a construction site to SAM and automatically output a high-quality, unlabeled segmentation mask (SAM mask). In this study, the number of points to be generated per side is set to 128, pred_iou_thresh is set to 0.9, and stability_score_thresh is set to 0.85. When a panoramic image of a construction site is input into Panoplane360, the output consists of two masks: one with labels for horizontal planes and the other with labels for vertical planes (refer to Fig. 3). Panoplane360 divides objects in the image into horizontal planes, vertical planes, and non-planar regions, and then generates segmentation masks for each. It estimates geometric information to determine the orientation of the planes and assigns labels to the segmentation masks based on this information.
3.2. Improve Masks Quality
As mentioned earlier, since SAM mask does not have a label, it is necessary to overlap the masks to assign the labels of the Panoplane360 object to the SAM object. However, the masks that come out of Panoplane360 are cropped at the top and bottom. Therefore, before overlapping each mask, the size of the SAM mask was cropped to the same size as the Panoplane360 mask. 
Overlapping was conducted by tallying the pixels of the label predicted by Panoplane360 in the corresponding locations of the object area on the cropped SAM mask, then assigning the label with the highest pixel count as the object's label. However, simply overlapping the SAM mask and Panoplane360 mask could result in assigning a label to the object on the SAM mask even if there is a slight overlap between the two masks. To mitigate this issue, we decided to retain only those objects where a single label occupies more than 50% of the total pixels within the object area. The following pseudo code, presented in Table 1, outlines the implementation process.

4. Experiments
The evaluation utilized a panoramic image of an apartment construction site in Korea captured with an Insta360 X2 camera, having an original resolution of 1024x512. Additionally, three images were categorized into hallway, window, and stairwell based on the construction site environment. We manually annotated the vertical-plane (v-plane) and horizontal-plane (h-plane) to generate ground truth masks for each image and compiled these into an evaluation dataset alongside the original image.
Using the test dataset, we computed the Mean Intersection over Union (MIoU) for all labels. The Panoplane360 model yielded an MIoU of 0.696, while the SAM+Panoplane360 model achieved an MIoU of 0.714, demonstrating that our approach combining SAM and Panoplane360 can enhance the quality of segmentation masks. Table 2 presents the MIoU calculated for h-plane and v-plane labels for each image.
For the v-plane, this method demonstrated superior quality across all test images. However, with the h-plane, we observed that SAM misclassified the object as background and thus did not assign a label, which had a negative impact on the results at image 2(refer to Fig. 4).

5. Conclusion
This research proposed a method to enhance segmentation results for panoramic construction site images without requiring a large, dedicated image dataset. The evaluation demonstrated an improvement in segmentation performance, with a score of 0.696 using Panoplane360 alone and 0.714 using the proposed method. The significance of this method lies in its ability to improve deep learning model performance despite the inherent data scarcity challenge in the construction domain. The proposed method offers versatility, as it can be combined not only with Panoplane360 but also with other deep learning models for various tasks. This versatility suggests its potential for broad applicability in construction site image analysis. Further research will focus on improving segmentation results. We will explore effective strategies for handling scenarios where the SAM fails to segment objects, especially cases where SAM misclassifies objects as background.

6. Acknowledgements
This research was supported by a grant (RS-2022-00143493) from Digital-Based Building Construction and Safety Supervision Technology Research Program funded by Ministry of Land, Infrastructure and Transport of Korean Government and National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 1711191456).







